# Building and Running the IoT Sample Solution

This document describes how to build and run the sample IoT solution. The solution simulates a large number of devices sending environmental readings for apartments in a smart building (for more information, see the [scenario](https://github.com/mspnp/iot-journey/blob/master/docs/journal/00-introducing-the-journey.md)). The simulator generates events that are posted to an Azure event hub. Azure Stream Analytics is used to process these events and save the results to blob storage. Optionally, the data can subsequently be processed by using a Hive query running on a Hadoop cluster. The query calculates the number of events generated by each device. 

***ADD SPIEL ABOUT THE COLD STORAGE PROCESSOR***

The simulator enables you to test the following scenarios:

1. All devices sending readings that are within the expected range. These readings simply need to be recorded so that they can be displayed on a dashboard or used for further analysis.

2. Some devices send readings indicating that the temperature is excessive (over 30 degrees Celsius). As well as being recorded, these readings may prompt some more immediate action.

You can download the code for the solution [here](https://github.com/mspnp/iot-journey/tree/master/src).

You should also download the [provisioning scripts](https://github.com/mspnp/iot-journey/tree/master/provision) that create the Azure assets required by the solution.

The solution comprises the following projects:

- ScenarioSimulator.ConsoleHost. This project contains the logic that implements each of the scenarios. You can configure the parameters used by the simulator (number of devices, simulation duration, warm up time, and so on) by editing the mysettings.config file in this project.

- Devices.Events. This project defines the events that the simulated devices for each of the scenarios can raise.

- ScenarioSimulator.ConsoleHost. This project provides the user interface to the simulator. It implements a menu that enables the user to run each of the scenarios described above.

- ColdStorage.ConsoleHost and ColdStorage. These projects implement a cold storage processor. This processor ... **EXPLAIN WHAT IT DOES**

- Core. This project implements generic, reusable logic that is referenced by the other projects in the solution.

- ScenarioSimulator.Tests, ColdStorage.Tests and Core.Tests (in the UnitTests folder). These projects contain unit tests to verify that the simulator and cold storage processor are configured and running correctly.

## Prerequisites

To use the sample solution, you must have an Azure account with a subscription. You should also have installed the following software on your computer:

- [Visual Studio](https://www.visualstudio.com/)

- [Azure SDK for .NET (Visual Studio tools)](http://azure.microsoft.com/downloads/)

- [Azure Powershell](https://azure.microsoft.com/documentation/articles/powershell-install-configure/)

## Building the Sample Solution

To build and deploy the system, perform the following steps:

1. Download and install the NuGet packages and assemblies required by the solution.

2. Create the required Azure assets and services.

3. Verify the Stream Analytics configuration.

4. Verify the Azure SQL Database configuration.

5. Verify the HDInsight configuration.

6. Create the configuration file with your Azure account information and build the solution.

The following sections describe these steps in more detail.

### Downloading and Installing the Required Assemblies

- Using Visual Studio, open the IoTJourney solution.

- Rebuild the solution. This action downloads the various NuGet packages referenced by the solution and installs the appropriate assemblies.

> **Note:** The rebuild process should report an error: *Before building this project, please copy mysettings-template.config to mysettings.config ...* You will do this in a later step. If the rebuild process displays any other errors then clean the solution and rebuild it again.

### Creating the Required Azure Assets and Services

- Start Azure PowerShell as administrator.

> **Note:** Windows Powershell must be configured to run scripts. You can do this by running the following PowerShell command before continuing:
> 
> `set-executionpolicy unrestricted`

- Move to the folder containing the provisioning scripts.

- Run the following command:

	`.\Provision-All.ps1`

-  At the *SubscriptionName* prompt, enter the name of the subscription that you are using with your Azure account.

- At the *ServiceBusNamespace* prompt, enter the name of the Service Bus namespace you wish to create to hold the event hub.
 
- At the *StorageAccountName* prompt, enter the name of the storage account you wish to create to hold the data generated by the simulator.

- At the *SqlServerName* prompt, enter the name of the Azure SQL Database server you wish to create to hold the data for warm processing.

- At the *SqlDatabasePassword* prompt, enter a new password for the Azure SQL Database server.

- At the *HDInsightClusterName* prompt, enter the name of the HDInsight Cluster you wish to create to process event data.

- At the *ResourceGroup* prompt, enter the name of the resource group that will be used to contain the Azure SQL Database server.

- In the Sign in to Windows Azure Powershell dialog box, provide the credentials for your Azure account.

- In the Windows PowerShell credential request dialog box, enter a new password for the admin account for the HDInsight cluster to be created.

> **Note:** The password must be at least 10 characters long, contain a mixture up upper and lower case letters and numbers, and include at least one special character. If you fail to provide a sufficiently complex password, the provisioning script will fail to create the cluster and instead report a *PreClusterCreationValidationFailure*.

- Wait while the various resources are provisioned. This can take several minutes.

### Verifying the Stream Analytics Configuration

The provisioning script should configure Stream Analytics automatically to receive events from two input sources; the event hub and JSON data held in a container in blob storage. The stream analytics job should and write records of each event to blob storage using JSON format, and a summary showing the average temperature observed by all devices in each building during a 5 second rolling time window to a table in an Azure SQL Database.

To verify that the configuration was successful, perform the following steps:

- Using the Azure web portal, open the Stream Analytics page.

- On the Stream Analytics page, verify that a Stream Analytics job called fabrikamstreamjob01 has been created.

- Click the job, and then in the menu bar click Configure

- On the Configuration page, verify that the storage account is set to the value that you specified when you ran the provisioning script (the name that you entered in response to the *StorageAccountName* prompt).

- In the menu bar click Inputs.

- On the Inputs page, verify that two inputs named input01 and input02 have been created. The type of the input should be Event Hub and the type of input02 should be Blob storage.

- Click input01. 

- On the input01 page, in the General section, verify that:

	-  The service bus namespace is set to the value that you specified in response to the *ServiceBusNamespace* prompt when you ran the provisioning script.

	-  The event hub name is set to eventhub01.

	-  The event hub policy name is set to ManagePolicy.

	-  The event hub consumer group is set to consumergroup01.

- In the Serialization section, verify that the event serialization format is set to JSON.

- Return to the job page, and then click input02. 

- On the input02 page, in the General section, verify that:

	-  The storage account Bus namespace is set to the value that you specified in response to the *StorageAccountName* prompt when you ran the provisioning script.

	-  The container is set to container01refdata.

	-  The path pattern is set to fabrikam/buildingdevice.json.

- In the Serialization section, verify that the event serialization format is set to JSON.

- Return to the job page, and then in the menu bar click Query

- On the Query page, verify that the following queries are defined:
	```
	SELECT * INTO output01 FROM input01 TIMESTAMP BY TimeObserved;

	SELECT AVG(I1.Temperature) as Temperature, Max(I1.TimeObserved) as LastObservedTime, I2.BuildingId 
	INTO output02 
	FROM input01 I1 TIMESTAMP BY TimeObserved
	JOIN input02 I2 On I1.DeviceId = I2.DeviceId
	GROUP BY TumblingWindow(s,5), I2.BuildingId
	```

- In the menu bar click Outputs.

- On the Outputs page, verify that the following two outputs are defined:

	- output01 with an output type of Blob Storage

	- output02 with an output type of SQL Database

- Click output01. On the output01 page, in the General section, verify that:

	-  The storage account is set to the value that you specified in response to the *StorageAccountName* prompt when you ran the provisioning script.

	-  The container is set to container01.

	-  The filename prefix is set to fabrikam.

	- In the Serialization section, verify that the event serialization format is set to JSON.

- Return to the Outputs page.

- Click output02. On the output02 page verify that:

	- The server name is set to the value that you specified in response to the *sqlServerName* prompt when you ran the provisioning script.

	- The database name is set to fabrikamdb01.

	- The username is set to fabrikamdbuser01.
	
	- The tablename is set to BuildingTemperature.

### Verifying the Azure SQL Database Configuration

The provisioning script creates an Azure SQL Database to hold the rolling summary data of the average temperature in each building. Perform the following steps to verify the configuration of the database and server:

- Using the Azure web portal, open the SQL Databases page.

- Verify that a database names fabrikam01 has been successfully created and that it is located in the server that you specified in response to the *sqlServerName* prompt when you ran the provisioning script.

### Verifying the HDInsight Configuration

The provisioning script creates an HDInsight cluster that uses Hadoop services running a Hive query to process event data. Perform the following steps to verify the configuration of the HDInsight cluster:

- Using the Azure web portal, open the HDInsight page.

- Verify that the HDInsight cluster has been successfully created.

- Click the cluster. 

- In the menu bar click Dashboard.

- On the Dashboard page, verify that the cluster is linked to the storage account that you specified in response to the *StorageAccountName* prompt when you ran the provisioning script.

- In the menu bar, click Configuration.

- On the Configuration page, verify that Hadoop Services are enabled (On)

- In the menu bar, click Scale.

- On the Scale page, verify that the instance count is set to 2.

### Creating the Configuration File and Building the Solution

- Using File Explorer copy the file named *mysettings-template.config* in the RunFromConsole folder to *mysettings.config*

- In Visual Studio, open the *mysettings.config* file in the ScenarioSimulator.ConsoleHost project. 

- Using the Azure portal, find the connection string for the eventhub01 event hub in the Service Bus namespace that was created by the provisioning script.

- In Visual Studio, set the value of the Simulator.EventHubConnectionString key to the connection string for the eventhub01 event hub. Append the text `;TransportType=Amqp` to the end of the string.

> **Note:** It is important to specify the transport type because the default protocol is not supported by Azure Event Hub and will trigger exceptions at runtime.

- Set the value of the Simulator.EventHubPath key to eventhub01.

- Using the Azure portal, find the primary account key for the storage account that was created by the provisioning script.

- In Visual Studio, set the value of the Coldstorage.CheckpointStorageAccount key using the name of the storage account and the primary account key.

- Repeat the previous step to set the value of the Coldstorage.BlobWriterStorageAccount key.

- Using the Azure portal, find the connection string for the Service Bus namespace that was created by the provisioning script.

> **Note:** This is not the same as the event hub connection string retrieved earlier.

- In Visual Studio, set the value of the Coldstorage.EventHubConnectionString key to the connection string for the eventhub01 event hub. Append the text `;TransportType=Amqp` to the end of the string.

- Set the value of the Coldstorage.EventHubName key to eventhub01.

- (Optional) Modify the values of the other parameters, such as the number of devices or the duration of the simulation if required (the default values are perfectly acceptable, but you can experiment with different settings).

- Save the configuration file and rebuild the solution. It should now build successfully.

***There are a couple of config settings in the App.config files for the ColdStorage console and unit test projects that also need to be set - if they are not merged into mysettings.config then they need to be covered here***

## Testing and Running the Solution

The following sections describe how to test and run the simulator and cold storage processor.

### Running the Unit Tests

To run the unit tests, on the Visual Studio menu bar click Test, click Run, and then click All Tests. All tests should succeed without errors.

### Running the Simulator

Perform the following steps to run the simulator:

- Using the Azure web portal, open the Stream Analytics page.

- Select fabrikamstreamjob01, and on the command bar click Start.

- In the Start Output dialog box, select Job Start Time and then click the tick button.

- Wait for the Stream Analytics job to start before continuing.

- Using Visual Studio, run the ScenarioSimualtor.ConsoleHost project.

- On the menu, select option 1 (*Run NoErrorsExpected*). The simulator will generate events and echo their contents to the screen. The simulator should not report any exceptions.

- Allow the simulator to run for a few minutes and then press q.

- Press Enter to return to the menu.

- Repeat the previous three steps for option 2. As before, the simulator will generate and echo events. No exceptions should occur.

- On the menu, select option 3 to quit the simulator.

### Verifying the Stream Analytics output

Perform the following steps to verify that events are being processed correctly by Stream Analytics:

- Using the Visual Studio Server Explorer window, connect to your Azure account.

- Expand the Storage node, expand the node that corresponds to your storage account, expand Blobs, and verify that a container named container01 has been created (you may need to refresh the display if the container does not appear).

- Double-click container01 to display the container01 contents pane.

- On the container01 contents, verify that the container has a folder named fabrikam.

- Double-click the fabrikam folder and verify that it contains one or more JSON files (files with random name but with the .json suffix).

- Double-click the most recent file to download and view the contents. Verify that the file contains a number of JSON formatted event records (there should be one record for each event that was generated when the simulator ran, although you probably didn't count them at the time!)

> **Note:** The data for each event comprises the ID of the device that reported the temperature, the value of the temperature, the date and time at which the data was processed, the event hub partition used to process the data, and the date and time at which the data was received by the event hub. The same fields are captured for events raised by both scenarios.

### Running the Cold Storage Processor

***I AM NOT SURE THAT I UNDERSTAND WHAT THIS IS DOING, SO THESE STEPS COULD BE WRONG!!***
Perform the following steps to run the cold storage processor. 

1. Using Visual Studio, run the ColdProcessor.ConsoleHost project.

- On the menu, select option 1 (*Provision Resources*). The cold storage processor will check that the resources it requires are available, or create them if necessary.

- Press Enter to return to the menu.

- On the menu, select option 2 (*Run Cold Storage Consumer*).

- Allow the simulator to run for a few minutes and then press q.

- Press Enter to return to the menu.

- On the menu, select option 3 to quit the cold storage processor.

### Analyzing the Blob Data by Using a Hive Query

Perform the following steps to analyze the data in blob storage by using a Hive query:

- Start Azure PowerShell as administrator.

> **Note:** Windows Powershell must be configured to run scripts. You can do this by running the following PowerShell command before continuing:
> 
> `set-executionpolicy unrestricted`

- Move to the folder containing the source code for the solution scripts, and then move to the Validation\HDInsight subfolder.

- Run the following command:

	`.\hivequeryforstreamanalytics.ps1`

- At the *subscriptionName* prompt, enter the name of the subscription that  owns the storage account holding the blob data used by the simulator.

- At the *storageAccountName* prompt, enter the name of the storage account.

- At the *containerName* prompt, type container01 (this is the name of the container that the simulator uses to hold the blob data).

-  At the *clusterName* prompt, enter the name of the Hadoop cluster.

-  In the Sign in to Windows Azure Powershell dialog box, provide the credentials for your Azure account.

- Verify that the message Successfully connected to cluster *cluster name* appears (where *cluster name* is the name of your Hadoop cluster), and then wait for the Hive query to complete. The results should appear on the standard output. consisting of a series of pairs; the ID of a device and the number of events that the device generated.

### Analyzing the Data in Cold Storage by Using a Hive Query

Perform the following steps to analyze the data in blob storage by using a Hive query:

- Start Azure PowerShell as administrator.

> **Note:** Windows Powershell must be configured to run scripts. You can do this by running the following PowerShell command before continuing:
> 
> `set-executionpolicy unrestricted`

- Move to the folder containing the source code for the solution scripts, and then move to the Validation\HDInsight subfolder.

- Run the following command:

	`.\hivequeryforcoldstorageeventprocessor.ps1`

- At the *subscriptionName* prompt, enter the name of the subscription that  owns the storage account holding the blob data used by the simulator.

- At the *storageAccountName* prompt, enter the name of the storage account.

- At the *containerName* prompt, type container01 (this is the name of the container that the simulator uses to hold the blob data).

-  At the *clusterName* prompt, enter the name of the Hadoop cluster.

-  In the Sign in to Windows Azure Powershell dialog box, provide the credentials for your Azure account.

- Verify that the message Successfully connected to cluster *cluster name* appears (where *cluster name* is the name of your Hadoop cluster), and then wait for the Hive query to complete. 

***I AM NOT GETTING ANY RESULTS OTHER THAN THE JOB ID***

